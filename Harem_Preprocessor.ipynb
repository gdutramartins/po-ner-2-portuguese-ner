{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Harem-Preprocessor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjee45edqFiR"
      },
      "source": [
        "# Conversor Dataset Harem para JSON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hMC1bgaqQDu"
      },
      "source": [
        "Suporte as versões primeira, mini e segunda. Converte os arquivos no formato xml para json com anotações em span."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCtltTHuT1gQ"
      },
      "source": [
        "Baseado na implementação de Fabio Souza\n",
        "https://github.com/fabiocapsouza/harem_preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFwapPyfWVvL"
      },
      "source": [
        "from typing import Dict, List, Tuple, Union\n",
        "import logging\n",
        "import re\n",
        "import json\n",
        "\n",
        "from lxml import etree\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhWmP7n8qEsz",
        "outputId": "e27fe542-7c97-473a-cab4-88b3163580c5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGEdipkaWiVO"
      },
      "source": [
        "logger = logging.getLogger()\n",
        "\n",
        "ENTITY = Dict[str, Union[str, int]]\n",
        "DOCUMENT = Dict[str, Union[str, List[ENTITY]]]\n",
        "\n",
        "PUNCTUATION_NEED_SPACE = ['.','!',':',';','?',',']\n",
        "PUNCTUATION_NOT_NEED_SPACE = ['\"','#','$','%','&','\\'','(',')','*','+','-','/','','<','=','>','@','[','\\\\',']','^','_','`','{','|','}','~']\n",
        "\n",
        "\n",
        "GDRIVE_PATH:str = '/content/drive/MyDrive'\n",
        "DATASET_PRIMEIRO_HAREM_ORIGEM = os.path.join(GDRIVE_PATH, 'dataset', 'po-ner', '02-portuguese-ner', 'CDPrimeiroHAREMprimeiroevento.xml')\n",
        "DATASET_MINI_HAREM_ORIGEM = os.path.join(GDRIVE_PATH, 'dataset', 'po-ner', '02-portuguese-ner', 'CDPrimeiroHAREMMiniHAREM.xml')    \n",
        "DATASET_SEGUNDO_HAREM_ORIGEM = os.path.join(GDRIVE_PATH, 'dataset', 'po-ner', '02-portuguese-ner', 'CDSegundoHAREMReRelEM.xml')    \n",
        "DATASETS_HAREM=[DATASET_PRIMEIRO_HAREM_ORIGEM, DATASET_MINI_HAREM_ORIGEM, DATASET_SEGUNDO_HAREM_ORIGEM]\n",
        "DATASET_OUTPUT_PAHT = os.path.join(GDRIVE_PATH, 'dataset', 'po-ner', '02-portuguese-ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQaU-tzGW2xF"
      },
      "source": [
        "\"\"\"These utility functions are copied from HuggingFace Transformers Library.\n",
        "https://github.com/huggingface/transformers/blob/master/src/transformers/tokenization_bert.py\n",
        "\"\"\"\n",
        "import unicodedata\n",
        "\n",
        "# Utility functions below are copied from HugginFace Transformers.\n",
        "def _is_whitespace(char: str) -> bool:\n",
        "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "    # as whitespace since they are generally considered as such.\n",
        "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat == \"Zs\":\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_control(char: str) -> bool:\n",
        "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "    # These are technically control characters but we count them as whitespace\n",
        "    # characters.\n",
        "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return False\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"C\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char: str) -> bool:\n",
        "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "    cp = ord(char)\n",
        "    # We treat all non-letter/number ASCII as punctuation.\n",
        "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "    # Punctuation class but we treat them as punctuation anyways, for\n",
        "    # consistency.\n",
        "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"P\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def _is_whitespace_or_punctuation(char: str) -> bool:\n",
        "    return _is_whitespace(char) or _is_punctuation(char)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb2Pg71KW4JW"
      },
      "source": [
        "SELECTIVE_CATEGS = [\n",
        "    'PESSOA',\n",
        "    'ORGANIZACAO',\n",
        "    'LOCAL',\n",
        "    'TEMPO',\n",
        "    'VALOR',\n",
        "]\n",
        "\n",
        "ALL_CATEGS = SELECTIVE_CATEGS + [\n",
        "    'ABSTRACCAO',\n",
        "    'ACONTECIMENTO',\n",
        "    'COISA',\n",
        "    'OBRA',\n",
        "    'OUTRO',\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HqELHdpW8aX"
      },
      "source": [
        "class HypothesisViolation(Exception):\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h4krEITXBxZ"
      },
      "source": [
        "class HaremConverter:\n",
        "    \"\"\"Converts First HAREM XML format to JSON.\n",
        "    \n",
        "    Args:\n",
        "        selective (bool): turns on selective scenario, where only named\n",
        "            entities of tags PESSOA, ORGANIZACAO, LOCAL, TEMPO and VALOR are\n",
        "            considered. Defaults to False.\n",
        "        alt_strategy (str): the strategy used to select the final alternative\n",
        "            when dealing with ALT tags. One of `most_entities` or\n",
        "            `entity_coverage`.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,\n",
        "                 selective: bool = False,\n",
        "                 alt_strategy: str = 'most_entities'):\n",
        "        if selective:\n",
        "            self._accepted_labels = SELECTIVE_CATEGS\n",
        "        else:\n",
        "            self._accepted_labels = ALL_CATEGS\n",
        "        \n",
        "        strategies = ('most_entities', 'entity_coverage')\n",
        "        if alt_strategy not in strategies:\n",
        "            raise ValueError('`alt_strategy` must be one of {}'.format(strategies))\n",
        "        self.alt_strategy = alt_strategy\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _shift_offset(entity: ENTITY, group_offset: int) -> ENTITY:\n",
        "        \"\"\"Shifts start_offset and end_offset by `group_offset` characters.\"\"\"\n",
        "        entity['start_offset'] += group_offset\n",
        "        entity['end_offset'] += group_offset\n",
        "        return entity\n",
        "    \n",
        "    def _get_label(self, entity: etree._Element) -> Union[str, None]:\n",
        "        \"\"\"Gets the label of an entity considering the label scenario.\n",
        "        In case of ambiguity, returns the first acceptable label or None\n",
        "        if there are no acceptable labels.\"\"\"\n",
        "        categ = entity.attrib.get('CATEG')\n",
        "        if categ is None:\n",
        "            logger.debug('Could not find label of entity with attributes %s',\n",
        "                         dict(entity.attrib))\n",
        "            return None\n",
        "        \n",
        "        labels = [label.strip() for label in categ.split('|')]\n",
        "        for label in labels:\n",
        "            if label in self._accepted_labels:\n",
        "                return label\n",
        "        \n",
        "        logger.debug('Ignoring <EM ID=\"%s\" CATEG=\"%s\">.',\n",
        "                     entity.attrib.get(\"ID\"),\n",
        "                     categ)\n",
        "        return None\n",
        "    \n",
        "    def _convert_entity(self, elem: etree._Element) -> ENTITY:\n",
        "        \"\"\"Convert an <EM/> tag into a dict with the relevant information\n",
        "        considering the label scenario.\"\"\"\n",
        "        entity_text = self._get_clean_text(elem.text.lstrip())\n",
        "        \n",
        "        return {\n",
        "            'entity_id': elem.attrib['ID'],\n",
        "            'text': entity_text,\n",
        "            'label': self._get_label(elem),\n",
        "            'start_offset': 0,\n",
        "            'end_offset': len(entity_text),\n",
        "        }\n",
        "\n",
        "\n",
        "    def _iterate_alt_tag(self, alt_tag: etree._Element\n",
        "                        ) -> Tuple[str, List[ENTITY]]:\n",
        "        \"\"\"Iterate over an ALT tag and return the complete text and all\n",
        "        entities inside it as if it was a single alternative.\"\"\"\n",
        "        text = ''\n",
        "        entities = []\n",
        "\n",
        "        alt_tag_text = self._get_clean_text(alt_tag.text)\n",
        "        if alt_tag_text:\n",
        "            text += alt_tag_text \n",
        "        \n",
        "        for tag in alt_tag:\n",
        "            if tag.tag == 'EM':\n",
        "                entity = self._convert_entity(tag)\n",
        "                if entity['label'] is not None:\n",
        "                    self._shift_offset(entity, len(text))\n",
        "                    entities.append(entity)\n",
        "                text = self.append_text_safe(text, entity['text'])\n",
        "                \n",
        "                tag_tail = self._get_clean_text(tag.tail)\n",
        "                if tag_tail:\n",
        "                    text = self.append_text_safe(text, tag_tail)\n",
        "\n",
        "        return text, entities\n",
        "\n",
        "    def _split_alternatives(self,\n",
        "                            alt_text: str,\n",
        "                            alt_entities: List[ENTITY],\n",
        "                            ) -> Tuple[List[str], List[List[ENTITY]]]:\n",
        "        \"\"\"Given the text of an ALT tag and all entities inside it, divide the\n",
        "        text and entities of the distinct alternatives inside ALT.\n",
        "        \n",
        "        Example of ALT tag:\n",
        "            <ALT>Nomes de Origem|<EM ID=\"2011\" {...}>Nomes de Origem</EM></ALT>\n",
        "            \n",
        "            `alt_text` is \"Nomes de Origem|Nomes de Origem\"\n",
        "            `alt_entities` should be [{\n",
        "                'entity_id': 2011,\n",
        "                'start_offset': 16,\n",
        "                'end_offset': 31,\n",
        "                {...}\n",
        "            }]\n",
        "            Result is:\n",
        "                (['Nomes de Origem', 'Nomes de Origem'],  # Texts\n",
        "                 [\n",
        "                     [],  # No entities for first alternative\n",
        "                     [{\n",
        "                         'entity_id': 2011,\n",
        "                         'text': 'Nomes de Origem',\n",
        "                         'start_offset': 0,\n",
        "                         'end_offset': 15,\n",
        "                         'label': '...',  # label etc\n",
        "                     }]\n",
        "                 ])\n",
        "        \"\"\"\n",
        "        # Split the alternative solutions\n",
        "        alt_texts = alt_text.split('|')\n",
        "        if len(alt_texts) < 2:\n",
        "            raise HypothesisViolation(\n",
        "                \"ALT tag must have at least 2 alternatives.\")\n",
        "        \n",
        "        # Find the char offset of all \"|\" chars\n",
        "        divs = [div.start() for div in re.finditer(r'\\|', alt_text)]\n",
        "        \n",
        "        # Split entities into groups of the distinct alternatives.\n",
        "        # One group will later be selected as the true labels.\n",
        "        groups = []\n",
        "        for _ in range(len(alt_texts)):\n",
        "            groups.append([])\n",
        "        \n",
        "        group_ix = 0\n",
        "        group_start_offset = 0\n",
        "        current_group_end = divs[0]\n",
        "        \n",
        "        for entity in alt_entities:\n",
        "            start = entity['start_offset']\n",
        "            \n",
        "            if start > current_group_end:\n",
        "                # Entity belongs to next alternative\n",
        "                group_ix += 1\n",
        "                group_start_offset = current_group_end + 1\n",
        "\n",
        "                if group_ix < len(divs):\n",
        "                    current_group_end = divs[group_ix]\n",
        "                elif group_ix == len(divs):\n",
        "                    current_group_end = len(alt_text)\n",
        "\n",
        "            # Shift entity to discard the offset due to the text of previous\n",
        "            # alternatives\n",
        "            entity = self._shift_offset(dict(entity), -group_start_offset)\n",
        "            groups[group_ix].append(entity)\n",
        "                \n",
        "        assert len(groups) == len(alt_texts)\n",
        "\n",
        "        return alt_texts, groups\n",
        "        \n",
        "    \n",
        "    def _handle_alt(self, alt_tag: etree._Element) -> Tuple[str, List[ENTITY]]:\n",
        "        \"\"\"Handle ALT tag separating all distinct alternative solutions and\n",
        "        then selecting an alternative using the chosen heuristic.\"\"\"\n",
        "\n",
        "        # Extract complete text and all entities inside ALT\n",
        "        tag_text, entities = self._iterate_alt_tag(alt_tag)\n",
        "        # Divide it into the distinct alternatives\n",
        "        alt_texts, groups = self._split_alternatives(tag_text, entities)\n",
        "        \n",
        "        # Choose one alternative (one of alt_text and one of groups) based on\n",
        "        # the selected ALT strategy\n",
        "        if self.alt_strategy == 'most_entities':\n",
        "            # Choose the first group that have the highest number of accepted\n",
        "            # labels\n",
        "            ents_per_group = [len(group) for group in groups]\n",
        "            assert sum(ents_per_group) == len(entities)\n",
        "            N_max = ents_per_group.index(max(ents_per_group))\n",
        "            chosen_entities = groups[N_max]\n",
        "            group_text = alt_texts[N_max]\n",
        "            if sum(ents_per_group) != ents_per_group[N_max]:\n",
        "                # More than 2 groups with entities\n",
        "                not_chosen = groups[:]\n",
        "                not_chosen.remove(chosen_entities)\n",
        "                logger.debug(\n",
        "                    'Choosing ALT %s over alternatives %s', \n",
        "                    chosen_entities,\n",
        "                    not_chosen)\n",
        "        else:\n",
        "            assert self.alt_strategy == 'entity_coverage'\n",
        "            # Choose the group whose entities cover more text\n",
        "            coverages = [sum(len(ent['text']) for ent in group)\n",
        "                         for group in groups]\n",
        "            N_max = coverages.index(max(coverages))\n",
        "            chosen_entities = groups[N_max]\n",
        "            group_text = alt_texts[N_max]\n",
        "        \n",
        "            if sum(coverages) != coverages[N_max]:\n",
        "                # More than 2 groups with entities\n",
        "                logger.debug('Choosing ALT %s over alternatives %s',\n",
        "                             chosen_entities,\n",
        "                             groups[:].remove(chosen_entities))\n",
        "        \n",
        "        return group_text, chosen_entities\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _avoid_word_agglutination(text: str, insertion: str) -> str:\n",
        "        \"\"\"Conditionally inserts one space at the end of `text` to avoid word\n",
        "        agglutination that would happen by concatenating `text` and `insertion`.\n",
        "        \"\"\"\n",
        "        if not text or not insertion:\n",
        "            return text\n",
        "                \n",
        "        #if not _is_whitespace_or_punctuation(text[-1]) \\\n",
        "        #        and not _is_whitespace_or_punctuation(insertion[0]):\n",
        "        if not _is_whitespace(text[-1]) and \\\n",
        "           not _is_whitespace_or_punctuation(insertion[0]) and \\\n",
        "           not text[-1] in PUNCTUATION_NOT_NEED_SPACE:\n",
        "            text += ' '\n",
        "\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def append_text_safe(text: str, piece: str) -> str:\n",
        "        \"\"\"Appends `piece` to `text`, conditionally inserting a space in between\n",
        "        if directly appending would cause agglutination of the last word of\n",
        "        `text` and first word of `piece`.\"\"\"\n",
        "\n",
        "        if text and len(text) > 0 and piece and len(piece) > 0:\n",
        "            #if not _is_whitespace_or_punctuation(text[-1]) and not _is_whitespace_or_punctuation(piece[0]):\n",
        "            if not _is_whitespace(text[-1]) and \\\n",
        "               not _is_whitespace_or_punctuation(piece[0]) and \\\n",
        "               not text[-1] in PUNCTUATION_NOT_NEED_SPACE:    \n",
        "                \n",
        "                text += ' '\n",
        "        \n",
        "        return text + piece\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _get_clean_text(text: str) -> str:\n",
        "        \"\"\" Retorna o texto limpo de caracteres indedesejáveis \"\"\"\n",
        "        if not text:\n",
        "            return text\n",
        "        text_ret = text.replace('\\n','')\n",
        "        text_ret = re.sub('\\s+', ' ', text_ret)\n",
        "        text_ret = text_ret.strip()\n",
        "        return text_ret\n",
        "\n",
        "\n",
        "    def _convert_tag(self, tag: etree._Element) -> Tuple[str, List[ENTITY]]:\n",
        "        \"\"\"Convert a tag to a dictionary with all the relevant info,\n",
        "        keeping alignment of extracted entities to the original text.\"\"\"\n",
        "        text = ''\n",
        "        entities = []\n",
        "\n",
        "        if tag.tag == 'EM':\n",
        "            entity = self._convert_entity(tag)\n",
        "            if entity['label'] is not None:\n",
        "                entities.append(entity)\n",
        "            text = entity['text']\n",
        "\n",
        "        elif tag.tag == 'ALT':\n",
        "            alt_text, alt_entities = self._handle_alt(tag)\n",
        "            text = alt_text\n",
        "            entities = alt_entities\n",
        "        \n",
        "        tag_tail = self._get_clean_text(tag.tail)\n",
        "        if tag_tail is not None:\n",
        "            text = self._avoid_word_agglutination(text, tag_tail)\n",
        "            text += tag_tail\n",
        "                \n",
        "        return text, entities\n",
        "\n",
        "\n",
        "    def convert_document(self, doc: etree._Element) -> DOCUMENT:\n",
        "        \"\"\"O primeiro Harem e mini Harem utilizam o texto dentro da tag <DOC>,\n",
        "            já o segundo Harem utiliza tag <P> dentro da tag <DOC>. Precisamos\n",
        "            tratar a profundidade onde o texto se encontra nas diferentes versões \n",
        "            do Harem.\n",
        "        \n",
        "        \"\"\"\n",
        "        \n",
        "        text = ''\n",
        "        entities = []\n",
        "        \n",
        "        if doc.tag != 'DOC':\n",
        "            raise ValueError(\"`convert_document` expects a DOC tag.\")\n",
        "        \n",
        "        if doc.text is not None:\n",
        "            # Initial text before any tag\n",
        "            text += self._get_clean_text(doc.text)\n",
        "        \n",
        "        for prim_nivel_tag in doc:\n",
        "            if prim_nivel_tag.tag == 'P': #segundo harem com tags <p> \n",
        "                tag_text_p = self._get_clean_text(prim_nivel_tag.text)\n",
        "                if tag_text_p:\n",
        "                    text = self.append_text_safe(text, tag_text_p)\n",
        "                for seg_nivel_tag in prim_nivel_tag:\n",
        "                    text = self._convert_document_parts(tag=seg_nivel_tag, text=text, entities=entities)\n",
        "                #algumas tags <P> podem não terminar com ponto, título por exemplo.\n",
        "                if not _is_punctuation(text[-1]):\n",
        "                    text += '.'\n",
        "            else: #primeiro e mini harem, sem tags <p>, texto direto no <doc> \n",
        "                text = self._convert_document_parts(tag=prim_nivel_tag, text=text, entities=entities)\n",
        "                \n",
        "        return {\n",
        "            'doc_id': doc.attrib['DOCID'],\n",
        "            'doc_text': ''.join(text),\n",
        "            'entities': entities,\n",
        "        }\n",
        "\n",
        "    def _convert_document_parts(self, tag: etree._Element, text: str,entities: List[ENTITY]):\n",
        "        \"\"\" Trata a conversão do formato Harem e MiniHarem (sem <p>) e Segundo Harem (com <p>) \"\"\"\n",
        "        tag_text, tag_entities = self._convert_tag(tag)\n",
        "        text = self._avoid_word_agglutination(text, tag_text)\n",
        "\n",
        "        # Entity start and end offsets are relative to begin of `tag`.\n",
        "        # Shift tag_entities by current doc text length.\n",
        "        for entity in tag_entities:\n",
        "            self._shift_offset(entity, len(text))\n",
        "\n",
        "        # If last character was not a whitespace or punctuation, add space\n",
        "        # to prevent that an entity contains a word only partially\n",
        "        if tag_text:\n",
        "            text = self.append_text_safe(text, tag_text)\n",
        "        \n",
        "        entities.extend(tag_entities)\n",
        "\n",
        "        return text\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def convert_xml(cls, xml: str, **kwargs) -> List[DOCUMENT]:\n",
        "        \"\"\"Read a HAREM XML file and convert it to a JSON list according to the\n",
        "        chosen label scenario and alt resolution strategy.\"\"\"\n",
        "        converter = cls(**kwargs)\n",
        "        tree = etree.parse(xml)\n",
        "        \n",
        "        docs = []\n",
        "        i = 0\n",
        "        for doc in tree.findall('//DOC'):\n",
        "            doc_info = converter.convert_document(doc)\n",
        "            docs.append(doc_info)\n",
        "        return docs\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xj5cpS8RXF3a",
        "outputId": "0a30d688-cf58-4e89-8944-516562647154"
      },
      "source": [
        "    for dsHarem in DATASETS_HAREM:\n",
        "        converted_data = HaremConverter.convert_xml(xml=dsHarem,\n",
        "                                    selective=True,\n",
        "                                    alt_strategy='most_entities')\n",
        "        output_file = os.path.join(DATASET_OUTPUT_PAHT, dsHarem.split('/')[-1].replace('xml', 'json'))        \n",
        "        if os.path.exists(output_file):\n",
        "            os.remove(output_file)\n",
        "        print(f'Writing output file to {output_file}')\n",
        "        with open(output_file, 'w', encoding='utf-8') as fd:\n",
        "            json.dump(converted_data, fd, ensure_ascii=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing output file to /content/drive/MyDrive/dataset/po-ner/02-portuguese-ner/CDPrimeiroHAREMprimeiroevento.json\n",
            "Writing output file to /content/drive/MyDrive/dataset/po-ner/02-portuguese-ner/CDPrimeiroHAREMMiniHAREM.json\n",
            "Writing output file to /content/drive/MyDrive/dataset/po-ner/02-portuguese-ner/CDSegundoHAREMReRelEM.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut2bMeItLlHR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}